{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"metodich5_new.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mPerw0uAPGbc","colab_type":"text"},"source":["# Урок 5. Рекуррентные нейронные сети\n","\n","## Практическое задание\n","\n","<ol>\n","    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ.</li>\n","    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n","    <li>* Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM</li>\n","    <li>* Предложите свои варианты решения проблемы исчезающего градиента в RNN</li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"n0Yt8YdqPGbv","colab_type":"text"},"source":["### 1. LSTM для IMDB\n"]},{"cell_type":"code","metadata":{"id":"lp_vvy0fPGbw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1598617616732,"user_tz":-180,"elapsed":6080,"user":{"displayName":"Dmitry Batorov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifSMgNKSYsc5yt8ovZkIdAQuV9SbZYF97rbcJgkg=s64","userId":"17955106284390703841"}},"outputId":"0ae0913e-e94f-49ce-f649-06661398e53a"},"source":["from __future__ import print_function\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import LSTM\n","from keras.datasets import imdb\n","from keras.regularizers import L1L2\n","from keras.optimizers import SGD, RMSprop, Adam\n","\n","max_features = 20000\n","maxlen = 80\n","\n","print('Загрузка данных...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'тренировочные последовательности')\n","print(len(x_test), 'тестовые последовательности')\n","\n","print('Pad последовательности (примеров в x единицу времени)')\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Загрузка данных...\n","25000 тренировочные последовательности\n","25000 тестовые последовательности\n","Pad последовательности (примеров в x единицу времени)\n","x_train shape: (25000, 80)\n","x_test shape: (25000, 80)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ClNa8C44uvS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1598619254514,"user_tz":-180,"elapsed":49854,"user":{"displayName":"Dmitry Batorov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifSMgNKSYsc5yt8ovZkIdAQuV9SbZYF97rbcJgkg=s64","userId":"17955106284390703841"}},"outputId":"a117b4ea-e3c8-4093-92c8-1e55fbaf0d83"},"source":["print('Построение модели...')\n","model = Sequential()\n","\n","model.add(Embedding(max_features, 128))\n","model.add(LSTM(\n","    64, \n","    dropout=0.2, \n","    bias_regularizer=L1L2(0.01, 0.01)))  # L1L2(0, 0.01)\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',  # SGD(learning_rate=0.01, momentum=0.9),\n","              metrics=['accuracy'])\n","\n","print('Процесс обучения...')\n","model.fit(x_train, y_train,\n","          batch_size=16,\n","          epochs=1,\n","          validation_data=(x_test, y_test))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Построение модели...\n","Процесс обучения...\n","1563/1563 [==============================] - 47s 30ms/step - loss: 0.8570 - accuracy: 0.8008 - val_loss: 0.3773 - val_accuracy: 0.8438\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7feab2e70b38>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"29gf1AEOIOcT","colab_type":"text"},"source":["Модель немного улучшилась, благодаря добавлению двойной регуляризации, так как, заметил, что модель сильно переобучалась.\n","Также пробовал менять коэффициент Dropout, что не привело к улучшению модели.\n","Аналогично другие оптимизаторы не привели к улучшению. Лучше работает Adam с дефолтными парамтерами в данной ситуации. "]},{"cell_type":"markdown","metadata":{"id":"cS3SYeExPGb0","colab_type":"text"},"source":["### 2. Генерация текста на основе книги Алисы в стране чудес"]},{"cell_type":"code","metadata":{"id":"9h6sMPC2PGb1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598619506936,"user_tz":-180,"elapsed":231065,"user":{"displayName":"Dmitry Batorov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifSMgNKSYsc5yt8ovZkIdAQuV9SbZYF97rbcJgkg=s64","userId":"17955106284390703841"}},"outputId":"b4568e3e-181f-4f21-fd0f-bb8103de8462"},"source":["import numpy as np\n","from keras.layers import Dense, Activation\n","from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n","from keras.models import Sequential\n","\n","\n","# построчное чтение из примера с текстом \n","with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n","    lines = []\n","    for line in _in:\n","        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n","        if len(line) == 0:\n","            continue\n","        lines.append(line)\n","text = \" \".join(lines)\n","chars = set([c for c in text])\n","nb_chars = len(chars)\n","\n","\n","# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n","# ID and a specific character. The numerical ID will correspond to a column\n","# ID и определенный символ. Numerical ID будет соответсвовать колонке\n","# число при использовании one-hot кодировки для представление входов символов\n","char2index = {c: i for i, c in enumerate(chars)}\n","index2char = {i: c for i, c in enumerate(chars)}\n","\n","# для удобства выберете фиксированную длину последовательность 10 символов \n","SEQLEN, STEP = 10, 1\n","input_chars, label_chars = [], []\n","\n","# конвертация data в серии разных SEQLEN-length субпоследовательностей\n","for i in range(0, len(text) - SEQLEN, STEP):\n","    input_chars.append(text[i: i + SEQLEN])\n","    label_chars.append(text[i + SEQLEN])\n","\n","\n","# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n","\n","X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n","y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n","for i, input_char in enumerate(input_chars):\n","    for j, ch in enumerate(input_char):\n","        X[i, j, char2index[ch]] = 1\n","    y[i, char2index[label_chars[i]]] = 1\n","\n","\n","# установка ряда метапамертров  для нейронной сети и процесса тренировки\n","BATCH_SIZE, HIDDEN_SIZE = 128, 128\n","NUM_ITERATIONS = 25\n","NUM_EPOCHS_PER_ITERATION = 1\n","NUM_PREDS_PER_EPOCH = 100\n","\n","\n","# Create a super simple recurrent neural network. There is one recurrent\n","# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n","# encoded input layer. This is followed by a Dense fully-connected layer\n","# across the set of possible next characters, which is converted to a\n","# probability score via a standard softmax activation with a multi-class\n","# cross-entropy loss function linking the prediction to the one-hot\n","# encoding character label.\n","\n","'''\n","Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n","'''\n","\n","model = Sequential()\n","model.add(\n","    LSTM(\n","        HIDDEN_SIZE,\n","        return_sequences=False,\n","        input_shape=(SEQLEN, nb_chars),\n","        dropout=0.1, \n","        bias_regularizer=L1L2(0.01, 0.01),\n","        unroll=True\n","    )\n",")\n","model.add(Dense(nb_chars))\n","model.add(Activation(\"softmax\"))\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n","\n","\n","# выполнение серий тренировочных и демонстрационных итераций \n","for iteration in range(NUM_ITERATIONS):\n","\n","    # для каждой итерации запуск передачи данных в модель \n","    print(\"=\" * 50)\n","    print(\"Итерация #: %d\" % (iteration))\n","    model.fit(\n","        X, \n","        y, \n","        batch_size=BATCH_SIZE, \n","        epochs=NUM_EPOCHS_PER_ITERATION)\n","\n","    # Select a random example input sequence.\n","    test_idx = np.random.randint(len(input_chars))\n","    test_chars = input_chars[test_idx]\n","\n","    # для числа шагов предсказаний использование текущей тренируемой модели \n","    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n","    print(\"Генерация из посева: %s\" % (test_chars))\n","    print(test_chars, end=\"\")\n","    for i in range(NUM_PREDS_PER_EPOCH):\n","\n","        # здесь one-hot encoding.\n","        X_test = np.zeros((1, SEQLEN, nb_chars))\n","        for j, ch in enumerate(test_chars):\n","            X_test[0, j, char2index[ch]] = 1\n","\n","        # осуществление предсказания с помощью текущей модели.\n","        pred = model.predict(X_test, verbose=0)[0]\n","        y_pred = index2char[np.argmax(pred)]\n","\n","        # вывод предсказания добавленного к тестовому примеру \n","        print(y_pred, end=\"\")\n","\n","        # инкрементация тестового примера содержащего предсказание\n","        test_chars = test_chars[1:] + y_pred\n","print()\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","==================================================\n","Итерация #: 0\n","1241/1241 [==============================] - 6s 5ms/step - loss: 3.3634\n","Генерация из посева: ar: she go\n","ar: she gon the she she she she she she she she she she she she she she she she she she she she she she she sh==================================================\n","Итерация #: 1\n","1241/1241 [==============================] - 6s 5ms/step - loss: 2.1335\n","Генерация из посева:  went on, \n"," went on, and the said the said the said the said the said the said the said the said the said the said the sa==================================================\n","Итерация #: 2\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.9761\n","Генерация из посева: rofits you\n","rofits you done the said the said the said the said the said the said the said the said the said the said the ==================================================\n","Итерация #: 3\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.8670\n","Генерация из посева: ng about, \n","ng about, and the dor suth the dor suth the dor suth the dor suth the dor suth the dor suth the dor suth the d==================================================\n","Итерация #: 4\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.7900\n","Генерация из посева: royal chil\n","royal child the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the dormouse the dor==================================================\n","Итерация #: 5\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.7311\n","Генерация из посева: miss, were\n","miss, were was a little said the dormouse the morse the project gutenberg-tm elect on the said the dormouse th==================================================\n","Итерация #: 6\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.6824\n","Генерация из посева: e unjust t\n","e unjust to the was not in a little said the dormouse to the was not in a little said the dormouse to the was ==================================================\n","Итерация #: 7\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.6431\n","Генерация из посева: , turning \n",", turning to herself it was to the could not in the could not in the could not in the could not in the could n==================================================\n","Итерация #: 8\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.6092\n","Генерация из посева:  all ornam\n"," all ornament to the was to the thing the morse that she was not in a long and the morse that she was not in a==================================================\n","Итерация #: 9\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.5801\n","Генерация из посева:  this mome\n"," this moment the mock turtle to the project gutenberg-tm electronic works and the caterpillar of the sand and ==================================================\n","Итерация #: 10\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.5568\n","Генерация из посева:  pglaf), o\n"," pglaf), on the some of the some of the some of the some of the some of the some of the some of the some of th==================================================\n","Итерация #: 11\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.5342\n","Генерация из посева: e began, f\n","e began, for the share when she had not and the share when she had not and the share when she had not and the ==================================================\n","Итерация #: 12\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.5149\n","Генерация из посева:  some more\n"," some more that she had not a little sirtle the mock turtle some of the seated to herself the mock turtle some==================================================\n","Итерация #: 13\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4975\n","Генерация из посева: y lonely a\n","y lonely and the white rabbit all the project gutenberg-tm electronic work in a long a long and the white rabb==================================================\n","Итерация #: 14\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4837\n","Генерация из посева: may as wel\n","may as well you can do not in the project gutenberg-tm electronic work in a long said to herself to see the sh==================================================\n","Итерация #: 15\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4673\n","Генерация из посева: s history,\n","s history, and the dormouse to the triel her head the dormouse to the triel her head the dormouse to the triel==================================================\n","Итерация #: 16\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4587\n","Генерация из посева: e she was \n","e she was not alice was not alice was not alice was not alice was not alice was not alice was not alice was no==================================================\n","Итерация #: 17\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4461\n","Генерация из посева: nose, much\n","nose, much as she was a little share the project gutenberg-tm electronic works in the party a pig, and the que==================================================\n","Итерация #: 18\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4342\n","Генерация из посева: milk at te\n","milk at tee words and the project gutenberg-tm electronic works in the project gutenberg-tm electronic works i==================================================\n","Итерация #: 19\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4253\n","Генерация из посева: ck turtle \n","ck turtle said, said the king said to herself had the door of the sea, said the king said to herself had the d==================================================\n","Итерация #: 20\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4161\n","Генерация из посева: rly forgot\n","rly forgotten the mouse to the party at the project gutenberg-tm electronic works in the party at the project ==================================================\n","Итерация #: 21\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.4074\n","Генерация из посева: tch tell y\n","tch tell you with the trimes and the whole was the king said to herself the mock turtle sound the mock turtle ==================================================\n","Итерация #: 22\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.3983\n","Генерация из посева: tongue! sa\n","tongue! said the duchess to the copyright how the words and the white rabbit as she was not alice was not alic==================================================\n","Итерация #: 23\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.3912\n","Генерация из посева: ic work is\n","ic work is the works of the gryphon went on, and the gryphon went on, and the gryphon went on, and the gryphon==================================================\n","Итерация #: 24\n","1241/1241 [==============================] - 6s 5ms/step - loss: 1.3833\n","Генерация из посева:  and alice\n"," and alice was so of the trademark about it, said the king, and the queen said, and then all the reason and th\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JmSdISEIJmzY","colab_type":"text"},"source":["LSTM показала лучший результат. Также регуляризация и Dropout немного улучшили обучение. Но текст, конечно, все равно местами начинает повторять сам себя или фрагмент книги. Но, мне кажется, полностью от подобных эффектов избавиться не удастся, используя лишь рекррентные сети."]}]}